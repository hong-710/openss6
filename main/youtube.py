import pandas as pd
import re
import emoji
from googleapiclient.discovery import build
from tqdm import tqdm
from urllib.parse import urlparse, parse_qs


# -----------------------
# 1. API í‚¤ ì…ë ¥
# -----------------------
API_KEY = 'AIzaSyDEa7nG4YJuOyMyNxr4wcv47FyI0GlpUio'  # ë°œê¸‰ ë°›ì€ í‚¤ ë„£ê¸°! ë°˜ë“œì‹œ ìˆ˜ì •!


# -----------------------
# 2. video_id ì¶”ì¶œ í•¨ìˆ˜
# -----------------------
def extract_video_id(url):
    parsed_url = urlparse(url)
    if parsed_url.hostname == 'youtu.be':
        return parsed_url.path[1:]
    elif 'youtube.com' in parsed_url.hostname:
        query = parse_qs(parsed_url.query)
        return query.get('v', [None])[0]
    else:
        return None


# -----------------------
# 3. ëŒ“ê¸€ ìˆ˜ì§‘ í•¨ìˆ˜
# -----------------------
def get_comments(video_id, max_comments=500):
    youtube = build('youtube', 'v3', developerKey=API_KEY)
    comments = []
    next_page_token = None

    while len(comments) < max_comments:
        request = youtube.commentThreads().list(
            part="snippet",
            videoId=video_id,
            maxResults=100,
            pageToken=next_page_token,
            textFormat="plainText"
        )
        response = request.execute()

        for item in response['items']:
            comment = item['snippet']['topLevelComment']['snippet']
            comments.append({
                'text': comment['textDisplay'],
                'published_at': comment['publishedAt']
            })

        if 'nextPageToken' in response:
            next_page_token = response['nextPageToken']
        else:
            break

    return pd.DataFrame(comments)


# -----------------------
# 4. ì „ì²˜ë¦¬ í•¨ìˆ˜
# -----------------------
def load_stopwords(filepath):
    return pd.read_csv(filepath, header=None)[0].tolist()

def clean_text(text, stopwords):
    text = emoji.replace_emoji(text, replace='')              # ì´ëª¨ì§€ ì œê±°
    text = re.sub(r'[^\w\s]', '', text)                       # íŠ¹ìˆ˜ë¬¸ì ì œê±°
    text = re.sub(r'\s+', ' ', text)                          # ê³µë°± ì •ë¦¬
    text = text.lower()                                       # ì†Œë¬¸ìí™”
    words = [word for word in text.split() if word not in stopwords]
    return ' '.join(words)

# -----------------------
# 5. ë©”ì¸ ì‹¤í–‰
# -----------------------
if __name__ == "__main__":
    tqdm.pandas()

    video_url = input("ìœ íŠœë¸Œ ì˜ìƒ URLì„ ì…ë ¥í•˜ì„¸ìš”: ")
    video_id = extract_video_id(video_url)

    if not video_id:
        print("âŒ ì˜¬ë°”ë¥´ì§€ ì•Šì€ ìœ íŠœë¸Œ ë§í¬ì…ë‹ˆë‹¤.")
        exit()

    print(f"âœ… video_id ì¶”ì¶œ ì„±ê³µ: {video_id}")
    print("ğŸ“¥ ëŒ“ê¸€ ìˆ˜ì§‘ ì¤‘...")

    df = get_comments(video_id)

    if df.empty:
        print("âŒ ëŒ“ê¸€ì´ ì—†ê±°ë‚˜ ìˆ˜ì§‘ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        exit()

    df.to_csv("comments.csv", index=False, encoding='utf-8-sig')
    print("âœ… ëŒ“ê¸€ ìˆ˜ì§‘ ì™„ë£Œ! comments.csv ì €ì¥ë¨")

    print("ğŸ” stopwords.csv ë¡œ ë¶ˆìš©ì–´ ëª©ë¡ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...")
    stopwords = load_stopwords("stopwords.csv")

    print("ğŸ§¹ ì „ì²˜ë¦¬ ì¤‘...")
    df['cleaned'] = df['text'].progress_apply(lambda x: clean_text(x, stopwords))

    df.to_csv("cleaned_comments.csv", index=False, encoding='utf-8-sig')
    print("âœ… ì „ì²˜ë¦¬ ì™„ë£Œ! cleaned_comments.csv ì €ì¥ë¨")