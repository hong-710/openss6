import pandas as pd
import re
import emoji
from googleapiclient.discovery import build
from tqdm import tqdm
from urllib.parse import urlparse, parse_qs

from transformers import pipeline
import torch

# -----------------------
# 1. API í‚¤
# -----------------------
API_KEY = 'AIzaSyDpAjP_VVGIVQEfEpuS_GDV-VlvKWzyaMA'  # ê¼­ ë³¸ì¸ ê±¸ë¡œ!

# -----------------------
# 2. video_id ì¶”ì¶œ í•¨ìˆ˜
# -----------------------
def extract_video_id(url):
    parsed_url = urlparse(url)
    if parsed_url.hostname == 'youtu.be':
        return parsed_url.path[1:]
    elif 'youtube.com' in parsed_url.hostname:
        query = parse_qs(parsed_url.query)
        return query.get('v', [None])[0]
    else:
        return None

# -----------------------
# 3. ëŒ“ê¸€ ìˆ˜ì§‘ í•¨ìˆ˜
# -----------------------
def get_comments(video_id, max_comments=500):
    youtube = build('youtube', 'v3', developerKey=API_KEY)
    comments = []
    next_page_token = None

    while len(comments) < max_comments:
        request = youtube.commentThreads().list(
            part="snippet",
            videoId=video_id,
            maxResults=100,
            pageToken=next_page_token,
            textFormat="plainText"
        )
        response = request.execute()

        for item in response['items']:
            comment = item['snippet']['topLevelComment']['snippet']
            comments.append({
                'text': comment['textDisplay'],
                'published_at': comment['publishedAt']
            })

        if 'nextPageToken' in response:
            next_page_token = response['nextPageToken']
        else:
            break

    return pd.DataFrame(comments)

# -----------------------
# 4. ë¶ˆìš©ì–´ ë¡œë“œ & ì „ì²˜ë¦¬
# -----------------------
# -----------------------
# 4. ì „ì²˜ë¦¬ í•¨ìˆ˜
# -----------------------
def load_stopwords(filepath):
    return pd.read_csv(filepath, header=None)[0].tolist()

def clean_text(text, stopwords):
    text = emoji.replace_emoji(text, replace='')
    text = re.sub(r'[^\w\s]', '', text)
    text = re.sub(r'\s+', ' ', text)
    text = text.lower()
    words = [word for word in text.split() if word not in stopwords]
    return ' '.join(words)

# -----------------------
# 5. ì€ì–´ ì‚¬ì „ ë¡œë“œ
# -----------------------
slang_df = pd.read_csv("slang_dictionary_v3.csv", encoding="utf-8-sig")
slang_df.columns = slang_df.columns.str.strip()  # í˜¹ì‹œ ê³µë°± ìˆìœ¼ë©´ ì œê±°
slang_dict = dict(zip(slang_df['ë‹¨ì–´'], slang_df['ê°ì •']))

# -----------------------
# 6. ê°ì •ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì¤€ë¹„
# -----------------------
sentiment_pipeline = pipeline("sentiment-analysis", model="nlptown/bert-base-multilingual-uncased-sentiment")

# -----------------------
# 7. ê°ì • ë¶„ë¥˜ í•¨ìˆ˜
# -----------------------
def classify_sentiment(text):
    # ì€ì–´ ì‚¬ì „ ë¨¼ì € í™•ì¸
    for slang_word, sentiment in slang_dict.items():
        if slang_word in text:
            return sentiment

    # ì—†ìœ¼ë©´ transformers ëª¨ë¸ë¡œ ì˜ˆì¸¡
    try:
        result = sentiment_pipeline(text[:200])[0]  # ë„ˆë¬´ ê¸´ ë¬¸ì¥ì€ ì•ë¶€ë¶„ë§Œ
        label = result['label']
    except:
        return 'neutral'  # ì˜ˆì™¸ ì‹œ ì¤‘ë¦½ ì²˜ë¦¬

    # ë ˆì´ë¸” ë‹¨ìˆœí™”
    if '5' in label or '4' in label:
        return 'positive'
    elif '1' in label or '2' in label:
        return 'negative'
    else:
        return 'neutral'

# -----------------------
# 8. ë©”ì¸ ì‹¤í–‰
# -----------------------
if __name__ == "__main__":
    tqdm.pandas()

    video_url = input("ìœ íŠœë¸Œ ì˜ìƒ URLì„ ì…ë ¥í•˜ì„¸ìš”: ")
    video_id = extract_video_id(video_url)

    if not video_id:
        print("âŒ ì˜¬ë°”ë¥´ì§€ ì•Šì€ ìœ íŠœë¸Œ ë§í¬ì…ë‹ˆë‹¤.")
        exit()

    print(f"âœ… video_id ì¶”ì¶œ ì„±ê³µ: {video_id}")
    print("ğŸ“¥ ëŒ“ê¸€ ìˆ˜ì§‘ ì¤‘...")

    df = get_comments(video_id)

    if df.empty:
        print("âŒ ëŒ“ê¸€ì´ ì—†ê±°ë‚˜ ìˆ˜ì§‘ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        exit()

    df.to_csv("comments.csv", index=False, encoding='utf-8-sig')
    print("âœ… ëŒ“ê¸€ ìˆ˜ì§‘ ì™„ë£Œ! comments.csv ì €ì¥ë¨")

    print("ğŸ” stopwords.csv ë¡œ ë¶ˆìš©ì–´ ëª©ë¡ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...")
    stopwords = load_stopwords("stopwords.csv")

    print("ğŸ§¹ ì „ì²˜ë¦¬ ì¤‘...")
    df['cleaned'] = df['text'].progress_apply(lambda x: clean_text(x, stopwords))

    print("ğŸ’¬ ê°ì • ë¶„ë¥˜ ì¤‘...")
    df['sentiment'] = df['cleaned'].progress_apply(classify_sentiment)

    df.to_csv("cleaned_comments.csv", index=False, encoding='utf-8-sig')
    print("âœ… ì „ì²˜ë¦¬+ê°ì •ë¶„ì„ ì™„ë£Œ! cleaned_comments.csv ì €ì¥ë¨")



import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import matplotlib.font_manager as fm

df = pd.read_csv("cleaned_comments.csv", encoding="utf-8-sig")

# published_at ì»¬ëŸ¼ì„ datetimeìœ¼ë¡œ
df["published_at"] = pd.to_datetime(df["published_at"])
df["date"] = df["published_at"].dt.date
sentiment_counts = df["sentiment"].value_counts()

colors = ['skyblue', 'lightcoral', 'lightgray']
plt.figure(figsize=(6, 6))
plt.pie(sentiment_counts, labels=sentiment_counts.index, autopct='%1.1f%%', colors=colors)
plt.title("ê°ì • ë¹„ìœ¨ íŒŒì´ì°¨íŠ¸")
plt.tight_layout()
plt.show()

time_sentiment = df.groupby(["date", "sentiment"]).size().unstack().fillna(0)

plt.figure(figsize=(10, 5))
for col in time_sentiment.columns:
    plt.plot(time_sentiment.index, time_sentiment[col], label=col)

plt.title("ì‹œê°„ë³„ ê°ì • ë³€í™”")
plt.xlabel("ë‚ ì§œ")
plt.ylabel("ëŒ“ê¸€ ìˆ˜")
plt.legend()
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

text_data = " ".join(df["cleaned"].astype(str))  # cleaned ì»¬ëŸ¼ ì‚¬ìš©

font_path = "C:/Windows/Fonts/malgun.ttf"  # í•œê¸€í°íŠ¸ ê²½ë¡œ

wordcloud = WordCloud(font_path=font_path, background_color="white", width=800, height=400).generate(text_data)

plt.figure(figsize=(12, 6))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.title("ëŒ“ê¸€ ì›Œë“œí´ë¼ìš°ë“œ")
plt.tight_layout()
plt.show()

df.to_csv("cleaned_comments_with_graph.csv", index=False, encoding="utf-8-sig")


